<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">yudkowsky.net essays</title>
  <subtitle type="html">
    Essays by Eliezer S. Yudkowsky
  </subtitle>
  <updated>2008-09-18T05:02:28Z</updated>
  <id>tag:yudkowsky.net,2008-09-18:/feeds/essays.atom</id>
  <link rel="self" type="application/atom+xml" href="http://yudkowsky.net/feeds/essays.atom" />
  <rights>Copyright (c) 2008, Eliezer S. Yudkowsky</rights>
  <generator uri="http://radiantcms.org/">RadiantCMS</generator>
  <entry>
    <title>Cognitive Biases</title>
    <link rel="alternate" type="text/html" href="/rational/cognitive-biases"/>
    <id>tag:yudkowsky.net,2008-10-13:06/47/21Z:/rational/cognitive-biases</id>
    <updated>2011-11-10T23:19:47Z</updated>
    <published>2007-10-13T17:47:21Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
Introduces the field of heuristics and biases (the experimental investigation of systematic human errors and what they reveal about human cognition) from the perspective of how known biases may throw off our reasoning about uncertain risks to the human species.
&lt;br /&gt;&lt;a href=&quot;/rational/cognitive-biases&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>

  <entry>
    <title>Technical Explanation</title>
    <link rel="alternate" type="text/html" href="/rational/technical"/>
    <id>tag:yudkowsky.net,2008-10-09:01/04/19Z:/rational/technical</id>
    <updated>2011-05-20T04:19:23Z</updated>
    <published>2005-10-09T12:04:19Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
&lt;p&gt;More Bayes.  Many of my other writings rely on this page.&lt;/p&gt;&lt;br /&gt;&lt;a href=&quot;/rational/technical&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>Löb's Theorem</title>
    <link rel="alternate" type="text/html" href="/rational/lobs-theorem"/>
    <id>tag:yudkowsky.net,2008-10-13:05/04/22Z:/rational/lobs-theorem</id>
    <updated>2011-05-20T04:19:19Z</updated>
    <published>2008-10-13T16:05:16Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
Löb's Theorem shows that Peano Arithmetic can never assert its own soundness. I prove this amazing theorem using the standard mathematical technique of cartooning.&lt;br /&gt;&lt;a href=&quot;/rational/lobs-theorem&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>Twelve Virtues</title>
    <link rel="alternate" type="text/html" href="/rational/virtues"/>
    <id>tag:yudkowsky.net,2008-09-17:09/58/16Z:/rational/virtues</id>
    <updated>2011-04-04T06:23:43Z</updated>
    <published>2006-09-17T19:58:16Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
The first virtue is curiosity. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance.&lt;br /&gt;&lt;a href=&quot;/rational/virtues&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>Bayes' Theorem</title>
    <link rel="alternate" type="text/html" href="/rational/bayes"/>
    <id>tag:yudkowsky.net,2008-10-13:03/36/05Z:/rational/bayes</id>
    <updated>2010-08-17T01:25:50Z</updated>
    <published>2006-07-04T14:36:05Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
Bayes' Theorem for the curious and bewildered; an excruciatingly gentle introduction.&lt;br /&gt;&lt;a href=&quot;/rational/bayes&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>


  <entry>
    <title>Overcoming Bias</title>
    <link rel="alternate" type="text/html" href="/rational/overcoming-bias"/>
    <id>tag:yudkowsky.net,2008-11-26:02/36/05Z:/rational/overcoming-bias</id>
    <updated>2009-09-28T02:38:14Z</updated>
    <published>2008-11-26T02:46:31Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
&lt;p&gt;From August 2007 through May 2009, I blogged daily on the topic of human rationality at the econblog &lt;a href=&quot;http://www.overcomingbias.com/&quot;&gt;Overcoming Bias&lt;/a&gt; by Robin Hanson, getting around a quarter-million monthly pageviews.  This then forked off the community blog &lt;a href=&quot;http://lesswrong.com/&quot;&gt;Less Wrong&lt;/a&gt;, and I moved my old posts there as well for seed content (with &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt; forwarding, so don&amp;#8217;t worry if links are to overcomingbias.com).&lt;/p&gt;
&lt;p&gt;I suspected I could write faster by requiring myself to publish daily.  The experiment was a smashing success.&lt;/p&gt;
&lt;p&gt;Currently the &lt;i&gt;majority&lt;/i&gt; of all my writing is on Less Wrong.  To be notified when and if this material is compacted into e-books (or even physical books), &lt;a href=&quot;http://yudkowsky.net/subscribe/&quot;&gt;subscribe to this announcement list&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The material is heavily interdependent, and reading in chronological order may prove helpful:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.cs.auckland.ac.nz/~andwhay/postlist.html&quot;&gt;Andrew Hay&amp;#8217;s autogenerated index of all Yudkowsky posts in chronological order.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To see how interdependent it is, try looking over this graph of the dependency structure:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.cs.auckland.ac.nz/~andwhay/graphsfiles/dependencygraphs.html&quot;&gt;Andrew Hay&amp;#8217;s graphical visualization of major dependencies between Yudkowsky posts.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To read organized collections of posts, use the &lt;a href=&quot;http://wiki.lesswrong.com/wiki/Sequences&quot;&gt;Sequences&lt;/a&gt; on the Less Wrong wiki.&lt;/p&gt;
    </content>
  </entry>
  <entry>
    <title>The Simple Truth</title>
    <link rel="alternate" type="text/html" href="/rational/the-simple-truth"/>
    <id>tag:yudkowsky.net,2008-09-22:06/07/19Z:/rational/the-simple-truth</id>
    <updated>2008-10-29T01:44:53Z</updated>
    <published>2008-09-22T16:07:19Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
&lt;p&gt;What is &amp;#8220;truth&amp;#8221;?  It&amp;#8217;s surprisingly simple.&lt;/p&gt;&lt;br /&gt;&lt;a href=&quot;/rational/the-simple-truth&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>

  <entry>
    <title>AI and Global Risk</title>
    <link rel="alternate" type="text/html" href="/singularity/ai-risk"/>
    <id>tag:yudkowsky.net,2008-10-13:07/43/33Z:/singularity/ai-risk</id>
    <updated>2011-11-10T23:22:35Z</updated>
    <published>2007-10-13T18:43:33Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
How advanced artificial intelligence relates to global risk as both a potential catastrophe and a potential solution.  Contains considerable background material in cognitive sciences, and conveys much of my most recent views on intelligence, AI, and Friendly AI.&lt;br /&gt;&lt;a href=&quot;/singularity/ai-risk&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>Fun Theory</title>
    <link rel="alternate" type="text/html" href="/singularity/fun-theory"/>
    <id>tag:yudkowsky.net,2008-10-13:07/23/04Z:/singularity/fun-theory</id>
    <updated>2009-06-19T15:03:43Z</updated>
    <published>2002-01-25T18:23:04Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
&lt;p&gt;How much fun is there in the universe? What is the relation of available fun to intelligence? What kind of emotional architecture is necessary to have fun? Will eternal life be boring? Will we ever run out of fun?&lt;/p&gt;
&lt;p&gt;To answer questions like these… requires Singularity Fun Theory.&lt;/p&gt;&lt;br /&gt;&lt;a href=&quot;/singularity/fun-theory&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>Three Major Schools</title>
    <link rel="alternate" type="text/html" href="/singularity/schools"/>
    <id>tag:yudkowsky.net,2008-10-14:05/05/59Z:/singularity/schools</id>
    <updated>2008-10-24T02:27:39Z</updated>
    <published>2007-09-30T16:05:59Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
Singularity discussions seem to be splitting up into three major schools of thought: Accelerating Change, the Event Horizon, and the Intelligence Explosion.&lt;br /&gt;&lt;a href=&quot;/singularity/schools&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>Power of Intelligence</title>
    <link rel="alternate" type="text/html" href="/singularity/power"/>
    <id>tag:yudkowsky.net,2008-10-14:07/11/49Z:/singularity/power</id>
    <updated>2008-10-24T02:16:52Z</updated>
    <published>2007-07-10T18:11:54Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
&lt;p&gt;In our skulls we carry around 3 pounds of slimy, wet, greyish tissue, corrugated like crumpled toilet paper. You wouldn’t think, to look at the unappetizing lump, that it was some of the most powerful stuff in the known universe.&lt;/p&gt;&lt;br /&gt;&lt;a href=&quot;/singularity/power&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>Simplified Humanism</title>
    <link rel="alternate" type="text/html" href="/singularity/simplified"/>
    <id>tag:yudkowsky.net,2008-10-14:07/16/22Z:/singularity/simplified</id>
    <updated>2008-10-24T02:16:52Z</updated>
    <published>2007-06-16T18:16:22Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
If you believe professional bioethicists (people who get paid to explain ethical judgments) then the rule &quot;Life is good, death is bad; health is good, sickness is bad&quot; holds only until some critical age, and then flips polarity. Why should it flip? Why not just keep on with life-is-good?&lt;br /&gt;&lt;a href=&quot;/singularity/simplified&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>

  <entry>
    <title>The AI-Box Experiment</title>
    <link rel="alternate" type="text/html" href="/singularity/aibox"/>
    <id>tag:yudkowsky.net,2008-10-13:07/34/16Z:/singularity/aibox</id>
    <updated>2008-10-16T00:01:42Z</updated>
    <published>2002-07-05T18:34:16Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
&lt;p&gt;When we build AI, why not just keep it in sealed hardware that can&amp;#8217;t affect the outside world in any way except through one communications channel with the original programmers?&lt;br /&gt;
&amp;nbsp;That way it couldn&amp;#8217;t get out until we were convinced it was safe. Right?&lt;/p&gt;&lt;br /&gt;&lt;a href=&quot;/singularity/aibox&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>
  <entry>
    <title>5 Minute Intro</title>
    <link rel="alternate" type="text/html" href="/singularity/intro"/>
    <id>tag:yudkowsky.net,2008-10-14:07/04/05Z:/singularity/intro</id>
    <updated>2008-10-15T06:06:40Z</updated>
    <published>2007-05-26T18:04:05Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
This is a 5-minute spoken introduction to the Singularity I wrote for a small conference. I had to talk fast, though, so this is probably more like a 6.5 minute intro.&lt;br /&gt;&lt;a href=&quot;/singularity/intro&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>





  <entry>
    <title>Artifacts</title>
    <link rel="alternate" type="text/html" href="/other/fiction/artifacts"/>
    <id>tag:yudkowsky.net,2008-10-13:02/31/24Z:/other/fiction/artifacts</id>
    <updated>2008-10-17T02:25:19Z</updated>
    <published>2008-10-13T13:31:24Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
&lt;p&gt;In the western spiral arm of our galaxy lies a star system and a planet occupied ages ago. On one mountain of that planet there is a great structure, thousands of cubits tall&amp;#8230;&lt;/p&gt;&lt;br /&gt;&lt;a href=&quot;/other/fiction/artifacts&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>







  <entry>
    <title>Prospiracy Theory</title>
    <link rel="alternate" type="text/html" href="/other/fiction/prospiracy-theory"/>
    <id>tag:yudkowsky.net,2008-10-13:01/57/53Z:/other/fiction/prospiracy-theory</id>
    <updated>2008-10-24T02:35:01Z</updated>
    <published>2000-10-13T12:57:53Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
Out of habit, I identified the surveillance drones; a CIA sparrow, an FBI robin, a bluetit from the Men In Black, and a flock of honking ducks that was probably one of the Illuminati’s newfangled distributed devices...&lt;br /&gt;&lt;a href=&quot;/other/fiction/prospiracy-theory&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>

  <entry>
    <title>X17</title>
    <link rel="alternate" type="text/html" href="/other/fiction/X17"/>
    <id>tag:yudkowsky.net,2008-10-13:02/04/14Z:/other/fiction/X17</id>
    <updated>2008-10-24T02:35:01Z</updated>
    <published>1999-10-13T13:04:14Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
Short story inspired by &quot;doc&quot; Smith's _Lensman_ novels.&lt;br /&gt;&lt;a href=&quot;/other/fiction/X17&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>

  <entry>
    <title>Yehuda Yudkowsky</title>
    <link rel="alternate" type="text/html" href="/other/yehuda"/>
    <id>tag:yudkowsky.net,2008-10-13:03/10/08Z:/other/yehuda</id>
    <updated>2008-10-23T01:09:46Z</updated>
    <published>2008-10-13T14:10:07Z</published>
    <author>
      <name>Eliezer S. Yudkowsky</name>
      <uri>http://yudkowsky.net/</uri>
      <email>yudkowsky@gmail.com</email>
    </author>
    <content type="html">
Transhumanists are not fond of death. We would stop it if we could.&lt;br /&gt;&lt;a href=&quot;/other/yehuda&quot;&gt;More&lt;/a&gt;
    </content>
  </entry>

</feed>
