--- 
:title: yudkowsky.net essays
:items: 
- ident: tag:yudkowsky.net,2008-10-13:06/47/21Z:/rational/cognitive-biases
  description: |-
    Introduces the field of heuristics and biases (the experimental investigation of systematic human errors and what they reveal about human cognition) from the perspective of how known biases may throw off our reasoning about uncertain risks to the human species.
    <br /><a href="/rational/cognitive-biases">More</a>
  title: Cognitive Biases
  url: http://yudkowsky.net/rational/cognitive-biases
  time: "2011-11-10T23:19:47Z"
- ident: tag:yudkowsky.net,2008-10-09:01/04/19Z:/rational/technical
  description: <p>More Bayes.  Many of my other writings rely on this page.</p><br /><a href="/rational/technical">More</a>
  title: Technical Explanation
  url: http://yudkowsky.net/rational/technical
  time: "2011-05-20T04:19:23Z"
- ident: tag:yudkowsky.net,2008-10-13:05/04/22Z:/rational/lobs-theorem
  description: "L\xC3\xB6b's Theorem shows that Peano Arithmetic can never assert its own soundness. I prove this amazing theorem using the standard mathematical technique of cartooning.<br /><a href=\"/rational/lobs-theorem\">More</a>"
  title: "L\xC3\xB6b's Theorem"
  url: http://yudkowsky.net/rational/lobs-theorem
  time: "2011-05-20T04:19:19Z"
- ident: tag:yudkowsky.net,2008-09-17:09/58/16Z:/rational/virtues
  description: The first virtue is curiosity. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance.<br /><a href="/rational/virtues">More</a>
  title: Twelve Virtues
  url: http://yudkowsky.net/rational/virtues
  time: "2011-04-04T06:23:43Z"
- ident: tag:yudkowsky.net,2008-10-13:03/36/05Z:/rational/bayes
  description: Bayes' Theorem for the curious and bewildered; an excruciatingly gentle introduction.<br /><a href="/rational/bayes">More</a>
  title: Bayes' Theorem
  url: http://yudkowsky.net/rational/bayes
  time: "2010-08-17T01:25:50Z"
- ident: tag:yudkowsky.net,2008-11-26:02/36/05Z:/rational/overcoming-bias
  description: |-
    <p>From August 2007 through May 2009, I blogged daily on the topic of human rationality at the econblog <a href="http://www.overcomingbias.com/">Overcoming Bias</a> by Robin Hanson, getting around a quarter-million monthly pageviews.  This then forked off the community blog <a href="http://lesswrong.com/">Less Wrong</a>, and I moved my old posts there as well for seed content (with <span class="caps">URL</span> forwarding, so don&#8217;t worry if links are to overcomingbias.com).</p>
    <p>I suspected I could write faster by requiring myself to publish daily.  The experiment was a smashing success.</p>
    <p>Currently the <i>majority</i> of all my writing is on Less Wrong.  To be notified when and if this material is compacted into e-books (or even physical books), <a href="http://yudkowsky.net/subscribe/">subscribe to this announcement list</a>.</p>
    <p>The material is heavily interdependent, and reading in chronological order may prove helpful:</p>
    <ul>
    	<li><a href="http://www.cs.auckland.ac.nz/~andwhay/postlist.html">Andrew Hay&#8217;s autogenerated index of all Yudkowsky posts in chronological order.</a></li>
    </ul>
    <p>To see how interdependent it is, try looking over this graph of the dependency structure:</p>
    <ul>
    	<li><a href="http://www.cs.auckland.ac.nz/~andwhay/graphsfiles/dependencygraphs.html">Andrew Hay&#8217;s graphical visualization of major dependencies between Yudkowsky posts.</a></li>
    </ul>
    <p>To read organized collections of posts, use the <a href="http://wiki.lesswrong.com/wiki/Sequences">Sequences</a> on the Less Wrong wiki.</p>
  title: Overcoming Bias
  url: http://yudkowsky.net/rational/overcoming-bias
  time: "2009-09-28T02:38:14Z"
- ident: tag:yudkowsky.net,2008-09-22:06/07/19Z:/rational/the-simple-truth
  description: <p>What is &#8220;truth&#8221;?  It&#8217;s surprisingly simple.</p><br /><a href="/rational/the-simple-truth">More</a>
  title: The Simple Truth
  url: http://yudkowsky.net/rational/the-simple-truth
  time: "2008-10-29T01:44:53Z"
- ident: tag:yudkowsky.net,2008-10-13:07/43/33Z:/singularity/ai-risk
  description: How advanced artificial intelligence relates to global risk as both a potential catastrophe and a potential solution.  Contains considerable background material in cognitive sciences, and conveys much of my most recent views on intelligence, AI, and Friendly AI.<br /><a href="/singularity/ai-risk">More</a>
  title: AI and Global Risk
  url: http://yudkowsky.net/singularity/ai-risk
  time: "2011-11-10T23:22:35Z"
- ident: tag:yudkowsky.net,2008-10-13:07/23/04Z:/singularity/fun-theory
  description: "<p>How much fun is there in the universe? What is the relation of available fun to intelligence? What kind of emotional architecture is necessary to have fun? Will eternal life be boring? Will we ever run out of fun?</p>\n\
    <p>To answer questions like these\xE2\x80\xA6 requires Singularity Fun Theory.</p><br /><a href=\"/singularity/fun-theory\">More</a>"
  title: Fun Theory
  url: http://yudkowsky.net/singularity/fun-theory
  time: "2009-06-19T15:03:43Z"
- ident: tag:yudkowsky.net,2008-10-14:05/05/59Z:/singularity/schools
  description: "Singularity discussions seem to be splitting up into three major schools of thought: Accelerating Change, the Event Horizon, and the Intelligence Explosion.<br /><a href=\"/singularity/schools\">More</a>"
  title: Three Major Schools
  url: http://yudkowsky.net/singularity/schools
  time: "2008-10-24T02:27:39Z"
- ident: tag:yudkowsky.net,2008-10-14:07/11/49Z:/singularity/power
  description: "<p>In our skulls we carry around 3 pounds of slimy, wet, greyish tissue, corrugated like crumpled toilet paper. You wouldn\xE2\x80\x99t think, to look at the unappetizing lump, that it was some of the most powerful stuff in the known universe.</p><br /><a href=\"/singularity/power\">More</a>"
  title: Power of Intelligence
  url: http://yudkowsky.net/singularity/power
  time: "2008-10-24T02:16:52Z"
- ident: tag:yudkowsky.net,2008-10-14:07/16/22Z:/singularity/simplified
  description: If you believe professional bioethicists (people who get paid to explain ethical judgments) then the rule "Life is good, death is bad; health is good, sickness is bad" holds only until some critical age, and then flips polarity. Why should it flip? Why not just keep on with life-is-good?<br /><a href="/singularity/simplified">More</a>
  title: Simplified Humanism
  url: http://yudkowsky.net/singularity/simplified
  time: "2008-10-24T02:16:52Z"
- ident: tag:yudkowsky.net,2008-10-13:07/34/16Z:/singularity/aibox
  description: |-
    <p>When we build AI, why not just keep it in sealed hardware that can&#8217;t affect the outside world in any way except through one communications channel with the original programmers?<br />
    &nbsp;That way it couldn&#8217;t get out until we were convinced it was safe. Right?</p><br /><a href="/singularity/aibox">More</a>
  title: The AI-Box Experiment
  url: http://yudkowsky.net/singularity/aibox
  time: "2008-10-16T00:01:42Z"
- ident: tag:yudkowsky.net,2008-10-14:07/04/05Z:/singularity/intro
  description: This is a 5-minute spoken introduction to the Singularity I wrote for a small conference. I had to talk fast, though, so this is probably more like a 6.5 minute intro.<br /><a href="/singularity/intro">More</a>
  title: 5 Minute Intro
  url: http://yudkowsky.net/singularity/intro
  time: "2008-10-15T06:06:40Z"
- ident: tag:yudkowsky.net,2008-10-13:02/31/24Z:/other/fiction/artifacts
  description: <p>In the western spiral arm of our galaxy lies a star system and a planet occupied ages ago. On one mountain of that planet there is a great structure, thousands of cubits tall&#8230;</p><br /><a href="/other/fiction/artifacts">More</a>
  title: Artifacts
  url: http://yudkowsky.net/other/fiction/artifacts
  time: "2008-10-17T02:25:19Z"
- ident: tag:yudkowsky.net,2008-10-13:01/57/53Z:/other/fiction/prospiracy-theory
  description: "Out of habit, I identified the surveillance drones; a CIA sparrow, an FBI robin, a bluetit from the Men In Black, and a flock of honking ducks that was probably one of the Illuminati\xE2\x80\x99s newfangled distributed devices...<br /><a href=\"/other/fiction/prospiracy-theory\">More</a>"
  title: Prospiracy Theory
  url: http://yudkowsky.net/other/fiction/prospiracy-theory
  time: "2008-10-24T02:35:01Z"
- ident: tag:yudkowsky.net,2008-10-13:02/04/14Z:/other/fiction/X17
  description: Short story inspired by "doc" Smith's _Lensman_ novels.<br /><a href="/other/fiction/X17">More</a>
  title: X17
  url: http://yudkowsky.net/other/fiction/X17
  time: "2008-10-24T02:35:01Z"
- ident: tag:yudkowsky.net,2008-10-13:03/10/08Z:/other/yehuda
  description: Transhumanists are not fond of death. We would stop it if we could.<br /><a href="/other/yehuda">More</a>
  title: Yehuda Yudkowsky
  url: http://yudkowsky.net/other/yehuda
  time: "2008-10-23T01:09:46Z"
:website_url: 
